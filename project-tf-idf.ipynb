{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8e3b1c817d6a84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T20:40:30.775319Z",
     "start_time": "2024-03-28T20:39:54.151190Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "!pip install seaborn\n",
    "!pip install nltk\n",
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T20:40:44.329018Z",
     "start_time": "2024-03-28T20:40:30.783515Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "58be404e-eb80-479b-8ee5-633a73fea693",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import column\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler,OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "80e95008-2ae6-40c8-bc78-cc92118eb66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_spark():\n",
    "  return SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Big data project\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "19654b49-0406-435c-8ca1-0b30c6ab91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dataframe(dir=\"final_dataset.csv\"):\n",
    "    # Specify the directory where the CSV files are saved\n",
    "    csv_directory = dir\n",
    "    \n",
    "    # Read the CSV files back into a DataFrame\n",
    "    final_df_read = spark.read.option(\"header\", \"true\").csv(csv_directory)\n",
    "    \n",
    "    # Add a random column to shuffle data randomly\n",
    "    shuffled_df = final_df_read.withColumn(\"rand\", rand())\n",
    "    shuffled_df = shuffled_df.orderBy(\"rand\")\n",
    "    final_df_read = shuffled_df.drop(\"rand\")\n",
    "    return final_df_read\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "bfc7f32c-2b4e-463e-af66-2d6e3fa7972b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=get_data_dataframe(\"final_dataset4_imdb.csv\").limit(50000)\n",
    "df = df.filter(col(\"genre\") != \"thriller\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "96318541-5cdd-4628-a452-32ec642eaf7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41708"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a8c1bf1b-3c52-4f7c-9485-3c39bb138d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/umang/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/umang/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/umang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stopword = set(stopwords.words('english'))\n",
    "\n",
    "def preprocessing(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation using regular expressions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Remove specific characters #, @, and $\n",
    "    text = re.sub(r'[#@\\$]', '', text)\n",
    "    \n",
    "    # tokenize and convert to list\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    ## Lemmatize it \n",
    "    lemmatizer  = WordNetLemmatizer()\n",
    "    \n",
    "    ## lemmatize each token\n",
    "   # text = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    text = text.split()\n",
    "    \n",
    "    text = [word for word in text if word not in stopword]\n",
    "    \n",
    "    \n",
    "    return \" \".join(text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "d3312b75-5150-4bff-917f-f20cf13cbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdd=df.toPandas()\n",
    "# # pdd.drop_duplicates(inplace = True)\n",
    "# # pdd.dropna( inplace = True )\n",
    "# pdd.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "67e71c1d-ea5e-43e0-ae62-b592c7c5d314",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer, IDF, StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab36f563-9453-4c4a-80ed-bdceb3f04750",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "232f0be3-65ec-4978-a4f6-0fc0b8738442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=combined_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "358a2add-d7c9-4a4c-8b74-e715dc2e2a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "24c1171c-2197-4e1b-8807-e3cf72246aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate rows\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Remove rows with null values\n",
    "# df_no_null = df_no_duplicates.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "40158697-09ab-4cb8-a585-1d36049adb37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41708"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b97c2339-01f6-400a-9a1e-c12aefa016c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df=df_no_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "302eb5e8-9b6b-49db-b83c-682eab8786fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df.select([sum (col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "# # Show the count of null values for each column\n",
    "# df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "b43824c7-7de8-4c77-b2d8-631bf0eb668a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "78f91e52-916f-4acf-9dc7-177fd0642708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# df = df.select([sum (col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "\n",
    "# # Show the count of null values for each column\n",
    "# df.show()\n",
    "# Tokenization, stop word removal, and TF-IDF transformation for text data\n",
    "tokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "vectorizer = CountVectorizer(inputCol=\"filtered_words\", outputCol=\"raw_features\")\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"description_clean\")\n",
    "\n",
    "# Encoding genre labels\n",
    "genre_indexer = StringIndexer(inputCol=\"genre\", outputCol=\"label\")\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,remover,vectorizer, idf])\n",
    "transformed_train_df = pipeline.fit(df).transform(df)\n",
    "# Combine the preprocessed text features with the genre labels\n",
    "combined_train_df = genre_indexer.fit(transformed_train_df).transform(transformed_train_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f6dead36-bbb6-4ae1-b138-6ed713802b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=combined_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "20c2799e-0e3a-4a06-87c3-a8861a234f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 21:53:26 WARN DAGScheduler: Broadcasting large task binary with size 1102.4 KiB\n",
      "24/03/28 21:53:28 WARN DAGScheduler: Broadcasting large task binary with size 1102.4 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 33362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 219:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set count: 8342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Show the number of rows in each set\n",
    "print(\"Training set count:\", train_df.count())\n",
    "print(\"Testing set count:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "4ea2740a-8119-4bc7-abf4-22b7f4d379e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[movie_id: string, movie_name: string, genre: string, description: string, words: array<string>, filtered_words: array<string>, raw_features: vector, description_clean: vector, label: double]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "0500c0f4-61da-40a0-bbe1-4a89f0b99182",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 21:53:30 WARN DAGScheduler: Broadcasting large task binary with size 1083.6 KiB\n",
      "24/03/28 21:53:30 WARN DAGScheduler: Broadcasting large task binary with size 1083.7 KiB\n",
      "24/03/28 21:53:31 WARN DAGScheduler: Broadcasting large task binary with size 1483.6 KiB\n",
      "24/03/28 21:53:41 WARN DAGScheduler: Broadcasting large task binary with size 1903.7 KiB\n",
      "24/03/28 21:53:50 WARN MemoryStore: Not enough space to cache rdd_873_0 in memory! (computed 4.0 GiB so far)\n",
      "24/03/28 21:53:50 WARN BlockManager: Persisting block rdd_873_0 to disk instead.\n",
      "24/03/28 21:54:04 WARN MemoryStore: Not enough space to cache rdd_873_0 in memory! (computed 4.0 GiB so far)\n",
      "24/03/28 21:54:08 WARN DAGScheduler: Broadcasting large task binary with size 1961.5 KiB\n",
      "24/03/28 21:54:11 WARN MemoryStore: Not enough space to cache rdd_873_0 in memory! (computed 4.0 GiB so far)\n",
      "24/03/28 21:54:15 WARN DAGScheduler: Broadcasting large task binary with size 2041.5 KiB\n",
      "24/03/28 21:54:18 WARN MemoryStore: Not enough space to cache rdd_873_0 in memory! (computed 4.0 GiB so far)\n",
      "24/03/28 21:54:22 WARN DAGScheduler: Broadcasting large task binary with size 2.1 MiB\n",
      "24/03/28 21:54:25 WARN MemoryStore: Not enough space to cache rdd_873_0 in memory! (computed 4.0 GiB so far)\n",
      "24/03/28 21:54:29 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "24/03/28 21:54:33 WARN MemoryStore: Not enough space to cache rdd_873_0 in memory! (computed 4.0 GiB so far)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "# model = pipeline.fit(train_df)\n",
    "# Define the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(labelCol=\"label\", featuresCol=\"description_clean\", numTrees=50)\n",
    "\n",
    "# Train the Random Forest classifier\n",
    "model = rf_classifier.fit(df)\n",
    "\n",
    "# Make predictions\n",
    "# predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c77f1552-821b-4623-950c-e96054f1820f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 21:54:37 WARN DAGScheduler: Broadcasting large task binary with size 1364.5 KiB\n",
      "[Stage 243:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4381443298969072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictions = model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a725488e-9494-4994-856a-4b2bf2646346",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 21:54:40 WARN DAGScheduler: Broadcasting large task binary with size 1353.4 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(genre='crime', label=0.0, prediction=1.0),\n",
       " Row(genre='adventure', label=2.0, prediction=1.0),\n",
       " Row(genre='horror', label=3.0, prediction=2.0),\n",
       " Row(genre='action', label=4.0, prediction=1.0),\n",
       " Row(genre='adventure', label=2.0, prediction=1.0),\n",
       " Row(genre='adventure', label=2.0, prediction=4.0),\n",
       " Row(genre='adventure', label=2.0, prediction=1.0),\n",
       " Row(genre='adventure', label=2.0, prediction=0.0),\n",
       " Row(genre='horror', label=3.0, prediction=2.0),\n",
       " Row(genre='horror', label=3.0, prediction=1.0),\n",
       " Row(genre='action', label=4.0, prediction=2.0),\n",
       " Row(genre='adventure', label=2.0, prediction=1.0),\n",
       " Row(genre='crime', label=0.0, prediction=0.0),\n",
       " Row(genre='adventure', label=2.0, prediction=1.0),\n",
       " Row(genre='adventure', label=2.0, prediction=2.0),\n",
       " Row(genre='horror', label=3.0, prediction=1.0),\n",
       " Row(genre='adventure', label=2.0, prediction=1.0),\n",
       " Row(genre='action', label=4.0, prediction=2.0),\n",
       " Row(genre='romance', label=1.0, prediction=2.0),\n",
       " Row(genre='adventure', label=2.0, prediction=1.0),\n",
       " Row(genre='romance', label=1.0, prediction=1.0),\n",
       " Row(genre='romance', label=1.0, prediction=1.0),\n",
       " Row(genre='romance', label=1.0, prediction=1.0),\n",
       " Row(genre='adventure', label=2.0, prediction=1.0),\n",
       " Row(genre='romance', label=1.0, prediction=1.0),\n",
       " Row(genre='adventure', label=2.0, prediction=1.0),\n",
       " Row(genre='action', label=4.0, prediction=1.0),\n",
       " Row(genre='romance', label=1.0, prediction=1.0),\n",
       " Row(genre='horror', label=3.0, prediction=1.0),\n",
       " Row(genre='horror', label=3.0, prediction=0.0)]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.select(\"genre\",\"label\",\"prediction\").head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "bcf261ca-feec-44bf-a33b-d682d9c9014e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_statistis(predictions):\n",
    "\n",
    "    # Compute raw scores on the test set\n",
    "    predictionAndLabels = predictions.rdd.map(lambda lp: (lp.prediction, lp.label))\n",
    "    \n",
    "    # Instantiate metrics object\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "    \n",
    "    # Overall statistics\n",
    "    precision = metrics.precision(1.0)\n",
    "    recall = metrics.recall(1.0)\n",
    "    f1Score = metrics.fMeasure(1.0)\n",
    "    total_predictions = confusion_matrix.sum(axis=1)\n",
    "    print(\"Summary Stats\")\n",
    "    print(\"Precision = %s\" % precision)\n",
    "    print(\"Recall = %s\" % recall)\n",
    "    print(\"F1 Score = %s\" % f1Score)\n",
    "    \n",
    "    # Statistics by class\n",
    "    labels = predictions.rdd.map(lambda lp: lp.label).distinct().collect()\n",
    "    accuracies = {}\n",
    "    for label in sorted(labels):\n",
    "        # print(\"______________________\"+mapping[label]+\"_____________________\")\n",
    "        print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "        print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "        print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "        \n",
    "    \n",
    "    # Calculate the accuracy for each label\n",
    "    print(total_predictions)\n",
    "    for label in range(len(total_predictions)):\n",
    "        correct_predictions = confusion_matrix[label, label]\n",
    "        accuracy = correct_predictions / total_predictions[label]\n",
    "        accuracies[label] = accuracy\n",
    "    \n",
    "    # Print accuracies for each label\n",
    "    for label, accuracy in accuracies.items():\n",
    "        print(\"Accuracy for label %s: %s\" % (label, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "c2554d92-a669-412f-b66c-f2e716abf29e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/28 21:54:43 WARN DAGScheduler: Broadcasting large task binary with size 1380.4 KiB\n",
      "24/03/28 21:54:46 WARN DAGScheduler: Broadcasting large task binary with size 1390.5 KiB\n",
      "24/03/28 21:54:49 WARN DAGScheduler: Broadcasting large task binary with size 1383.1 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.3878048780487805\n",
      "Recall = 0.7522176227084565\n",
      "F1 Score = 0.5117682558841279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 253:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0.0 precision = 0.4775742261528743\n",
      "Class 0.0 recall = 0.45652173913043476\n",
      "Class 0.0 F1 Measure = 0.46681074405680767\n",
      "Class 1.0 precision = 0.3878048780487805\n",
      "Class 1.0 recall = 0.7522176227084565\n",
      "Class 1.0 F1 Measure = 0.5117682558841279\n",
      "Class 2.0 precision = 0.4024787997390737\n",
      "Class 2.0 recall = 0.3681384248210024\n",
      "Class 2.0 F1 Measure = 0.3845434714864444\n",
      "Class 3.0 precision = 0.5690737833594977\n",
      "Class 3.0 recall = 0.44505831798649476\n",
      "Class 3.0 F1 Measure = 0.4994832931450223\n",
      "Class 4.0 precision = 0.42410714285714285\n",
      "Class 4.0 recall = 0.16863905325443787\n",
      "Class 4.0 F1 Measure = 0.24132091447925486\n",
      "[1656. 1691. 1676. 1629. 1690.]\n",
      "Accuracy for label 0: 0.45652173913043476\n",
      "Accuracy for label 1: 0.7522176227084565\n",
      "Accuracy for label 2: 0.3681384248210024\n",
      "Accuracy for label 3: 0.44505831798649476\n",
      "Accuracy for label 4: 0.16863905325443787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print_statistis(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf89c81-d38a-4c6c-bca0-7358beca7c33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c03928-f265-4532-8411-ec1bc70f09ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ac2f12-b5c4-4c59-8b19-e5b10b7fe28f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eda8bf66d5b61f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-28T20:46:47.888154Z",
     "start_time": "2024-03-28T20:46:46.762814Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121b2c596795818d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
