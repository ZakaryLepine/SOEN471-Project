{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d05f44b2-ca7b-495e-b90b-fc5425ac1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import column\n",
    "from pyspark.sql.functions import *\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler,OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.mllib.util import MLUtils\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd4c8211-f7c2-406f-b538-31cb28e5d94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 17:13:55 WARN Utils: Your hostname, DESKTOP-968IKC4 resolves to a loopback address: 127.0.1.1; using 172.31.102.226 instead (on interface eth0)\n",
      "24/03/20 17:13:55 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/umang/.local/lib/python3.10/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.2.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/20 17:13:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def init_spark():\n",
    "  return SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Big data project\") \\\n",
    "        .config(\"spark.executor.memory\", \"8g\") \\\n",
    "        .config(\"spark.driver.memory\", \"8g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "spark = init_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8d0afec9-1ab2-4ef0-899a-75dbbf2e0b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dataframe(dir=\"final_dataset.csv\"):\n",
    "    # Specify the directory where the CSV files are saved\n",
    "    csv_directory = \"final_dataset.csv\"\n",
    "    \n",
    "    # Read the CSV files back into a DataFrame\n",
    "    final_df_read = spark.read.option(\"header\", \"true\").csv(csv_directory)\n",
    "    \n",
    "    # Add a random column to shuffle data randomly\n",
    "    shuffled_df = final_df_read.withColumn(\"rand\", rand())\n",
    "    shuffled_df = shuffled_df.orderBy(\"rand\")\n",
    "    final_df_read = shuffled_df.drop(\"rand\")\n",
    "    return final_df_read.limit(40000)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c866a22-762b-4cdf-aab3-c67a2a86026f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_class_distribution(df):\n",
    "    # Group the DataFrame by the \"genre\" column and count the occurrences of each genre\n",
    "    genre_counts = df.groupBy(\"genre\").count()\n",
    "    \n",
    "    # Show the genre counts\n",
    "    genre_counts.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23c2294-65a4-4cab-9f20-ebb6067b9781",
   "metadata": {},
   "source": [
    "# Replacing Null Values With Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc3cb1a2-1e27-4365-9b3e-909f10fe4c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_null_values_distribution(train_df):\n",
    "    null_value_list = list()\n",
    "    for col_ in train_df.columns:\n",
    "        null_value_list.append(train_df.filter(train_df[col_]==\"\\\\N\").count())                     \n",
    "    plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "    columns = [col_ for col_ in train_df.columns]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(columns, null_value_list, color='skyblue')\n",
    "    plt.xlabel('Columns')\n",
    "    plt.ylabel('Number of Null Values')\n",
    "    plt.ylim(0, train_df.count())\n",
    "    plt.title('Distribution of Null Values in Columns')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484237a5-5ec7-4f5e-9646-028b48bc7623",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64e3b0c0-90a4-42d1-a4aa-064faf15b830",
   "metadata": {},
   "source": [
    "# Drop Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "52be68e7-8c86-4253-bc9e-d50badc73172",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_duplicates(train_df):\n",
    "    print(train_df.count())\n",
    "    train_df=train_df.dropDuplicates()\n",
    "    print(train_df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97169abc-cd30-42ea-b7d9-d1e8373bdb90",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "38abba9c-cfb4-42d8-aef5-92f4f8517812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stages_of_normalization(categoricalColumns =['title',\n",
    "     'director',\n",
    "     'writer',\n",
    "     'actorPrimaryName'], predicting='genre'):\n",
    "    stages = []\n",
    "    for categoricalCol in categoricalColumns:\n",
    "        stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "        # encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], outputCols=[categoricalCol + \"classVec\"])\n",
    "        stages += [stringIndexer]\n",
    "    label_stringIdx = StringIndexer(inputCol = predicting, outputCol = 'label')\n",
    "    stages += [label_stringIdx]\n",
    "    \n",
    "    assemblerInputs = [c + \"Index\" for c in categoricalColumns] \n",
    "    assembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "    stages += [assembler]\n",
    "    return stages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "423b8ff6-7261-4f9b-b38d-4f0da0e9656d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_normalized_data(df):\n",
    "    pipeline = Pipeline(stages = stages)\n",
    "    pipelineModel = pipeline.fit(df)\n",
    "    final_df_read = pipelineModel.transform(df)\n",
    "    selectedCols = ['label', 'features','genre']+['title',\n",
    "     'director',\n",
    "     'writer',\n",
    "     'actorPrimaryName']\n",
    "    final_df_read = final_df_read.select(selectedCols)\n",
    "    final_df_read.printSchema()\n",
    "    return final_df_read"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fe7ea-5a77-4170-afba-2ac3f0f98718",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "878a1f12-572b-4c05-85ff-66692efc994c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_and_return_mapping_of_index_to_label(df,predicting='genre'):\n",
    "    # Collect distinct pairs of (label, genre)\n",
    "    label_genre_mapping = df.select(\"label\", predicting).distinct().collect()\n",
    "    map={}\n",
    "    # Print the mapping\n",
    "    for mapping in label_genre_mapping:\n",
    "        print(\"Label %s is mapped to genre '%s'\" % (mapping.label, mapping.genre))\n",
    "        map[mapping.label]= mapping.genre\n",
    "    return map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92aacf4e-86e6-4e8b-add2-fd96280f3062",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7dac7c-4234-4c90-8a64-c82badcae34b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf9bc5cd-b4b1-4933-9ffd-29849c7855e4",
   "metadata": {},
   "source": [
    "# Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0601470a-e89b-429f-93a2-4cd8d19701dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df_read=get_data_dataframe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "06162ff6-99a3-4707-9514-68971742361e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: double (nullable = false)\n",
      " |-- features: vector (nullable = true)\n",
      " |-- genre: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- director: string (nullable = true)\n",
      " |-- writer: string (nullable = true)\n",
      " |-- actorPrimaryName: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stages=get_stages_of_normalization()\n",
    "final_df_read=get_normalized_data(final_df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "914f1970-c394-41c4-9183-cbc826e2860b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 17:23:24 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 32011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 17:23:25 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing set count: 7989\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_df, test_df = final_df_read.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Show the number of rows in each set\n",
    "print(\"Training set count:\", train_df.count())\n",
    "print(\"Testing set count:\", test_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7136aabe-8c4c-4dd5-acba-1572103f6411",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 17:23:27 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "24/03/20 17:23:27 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "24/03/20 17:23:27 WARN DecisionTreeMetadata: DecisionTree reducing maxBins from 60000 to 32011 (= number of training instances)\n",
      "24/03/20 17:23:27 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "24/03/20 17:23:31 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n",
      "24/03/20 17:23:38 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "24/03/20 17:23:51 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "24/03/20 17:24:16 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(featuresCol = 'features', labelCol = 'label', maxBins=60000)\n",
    "dtModel = dt.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b971aece-0961-4255-8f66-4beba5ecd7ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 17:25:32 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+-----+--------+------------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|genre|   title|          director|            writer|    actorPrimaryName|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+-----+--------+------------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|[1.0,2234.0,2010....|Drama|    Vera|  Nedeljko Kovacic| Kristina Djukovic|         Nikola Kojo|[221.0,23.0,67.0,...|[0.68847352024922...|       0.0|\n",
      "|  0.0|[1.0,4372.0,4393....|Drama|    Vera|   Steffy Argelich|   Steffy Argelich|        John Steiner|[221.0,23.0,67.0,...|[0.68847352024922...|       0.0|\n",
      "|  0.0|[1.0,4372.0,4393....|Drama|    Vera|   Steffy Argelich|   Steffy Argelich|María Jose Gil Go...|[221.0,23.0,67.0,...|[0.68847352024922...|       0.0|\n",
      "|  0.0|[2.0,2266.0,2216....|Drama| Breathe|      Onur Karaman|      Onur Karaman|          Luis Oliva|[3.0,0.0,18.0,0.0...|[0.13043478260869...|       2.0|\n",
      "|  0.0|[3.0,147.0,124.0,...|Drama| Goliath|Adilkhan Yerzhanov|Adilkhan Yerzhanov|        Rabiya Abish|[28.0,31.0,13.0,6...|[0.30434782608695...|       1.0|\n",
      "|  0.0|[3.0,425.0,3260.0...|Drama| Goliath|  Frédéric Tellier|     Gaëlle Bellan|        Pierre Niney|[28.0,31.0,13.0,6...|[0.30434782608695...|       1.0|\n",
      "|  0.0|[4.0,875.0,179.0,...|Drama|    Rise|      Akin Omotoso|        Arash Amel|     Elijah Sholanke|[403.0,330.0,182....|[0.32951757972199...|       0.0|\n",
      "|  0.0|[9.0,3979.0,4002....|Drama|My House|Nick Norman-Butler|Nick Norman-Butler|       Francis Magee|[7218.0,4693.0,18...|[0.43613293051359...|       0.0|\n",
      "|  0.0|[19.0,3810.0,3830...|Drama|   Exile|     Martina Reese|     Martina Reese|      Marcin Kowalik|[403.0,330.0,182....|[0.32951757972199...|       0.0|\n",
      "|  0.0|[19.0,3810.0,3830...|Drama|   Exile|     Martina Reese|     Martina Reese|     Stanislaw Witek|[403.0,330.0,182....|[0.32951757972199...|       0.0|\n",
      "+-----+--------------------+-----+--------+------------------+------------------+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dtModel.transform(test_df)\n",
    "predictions.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3620aac-dbb5-4e7f-b1e5-3c97ad164e13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6f50abed-d065-4a18-8c92-df277735ab6f",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "15008c51-8730-45c5-94cf-1595a501237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_statistis(predictions,mapping):\n",
    "\n",
    "    # Compute raw scores on the test set\n",
    "    predictionAndLabels = predictions.rdd.map(lambda lp: (lp.prediction, lp.label))\n",
    "    \n",
    "    # Instantiate metrics object\n",
    "    metrics = MulticlassMetrics(predictionAndLabels)\n",
    "    confusion_matrix = metrics.confusionMatrix().toArray()\n",
    "    \n",
    "    # Overall statistics\n",
    "    precision = metrics.precision(1.0)\n",
    "    recall = metrics.recall(1.0)\n",
    "    f1Score = metrics.fMeasure(1.0)\n",
    "    total_predictions = confusion_matrix.sum(axis=1)\n",
    "    print(\"Summary Stats\")\n",
    "    print(\"Precision = %s\" % precision)\n",
    "    print(\"Recall = %s\" % recall)\n",
    "    print(\"F1 Score = %s\" % f1Score)\n",
    "    \n",
    "    # Statistics by class\n",
    "    labels = predictions.rdd.map(lambda lp: lp.label).distinct().collect()\n",
    "    accuracies = {}\n",
    "    for label in sorted(labels):\n",
    "        print(\"______________________\"+mapping[label]+\"_____________________\")\n",
    "        print(\"Class %s precision = %s\" % (label, metrics.precision(label)))\n",
    "        print(\"Class %s recall = %s\" % (label, metrics.recall(label)))\n",
    "        print(\"Class %s F1 Measure = %s\" % (label, metrics.fMeasure(label, beta=1.0)))\n",
    "        \n",
    "    \n",
    "    # Calculate the accuracy for each label\n",
    "    print(total_predictions)\n",
    "    for label in range(len(total_predictions)):\n",
    "        correct_predictions = confusion_matrix[label, label]\n",
    "        accuracy = correct_predictions / total_predictions[label]\n",
    "        accuracies[label] = accuracy\n",
    "    \n",
    "    # Print accuracies for each label\n",
    "    for label, accuracy in accuracies.items():\n",
    "        print(\"Accuracy for label %s: %s\" % (label, accuracy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10d4b124-98df-41fb-8efc-5fa083ef3a9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 17:29:54 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.40\n"
     ]
    }
   ],
   "source": [
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1f306afe-1117-4a93-a383-5a031d72e2e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label 0.0 is mapped to genre 'Drama'\n",
      "Label 1.0 is mapped to genre 'Comedy'\n",
      "Label 2.0 is mapped to genre 'Action'\n",
      "Label 3.0 is mapped to genre 'Horror'\n",
      "Label 4.0 is mapped to genre 'Thriller'\n",
      "Label 5.0 is mapped to genre 'Romance'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/03/20 17:29:57 WARN DAGScheduler: Broadcasting large task binary with size 2.7 MiB\n"
     ]
    }
   ],
   "source": [
    "mapp=print_and_return_mapping_of_index_to_label(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3d27693c-f564-449d-a4f9-60ec6931a64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/umang/.local/lib/python3.10/site-packages/pyspark/sql/context.py:125: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n",
      "24/03/20 17:29:58 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/03/20 17:29:58 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n",
      "24/03/20 17:31:03 WARN DAGScheduler: Broadcasting large task binary with size 3.3 MiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary Stats\n",
      "Precision = 0.3426724137931034\n",
      "Recall = 0.14058355437665782\n",
      "F1 Score = 0.19937304075235107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 125:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________Drama_____________________\n",
      "Class 0.0 precision = 0.4054972661445249\n",
      "Class 0.0 recall = 0.9137529137529138\n",
      "Class 0.0 F1 Measure = 0.5617195496417605\n",
      "______________________Comedy_____________________\n",
      "Class 1.0 precision = 0.3426724137931034\n",
      "Class 1.0 recall = 0.14058355437665782\n",
      "Class 1.0 F1 Measure = 0.19937304075235107\n",
      "______________________Action_____________________\n",
      "Class 2.0 precision = 0.5238095238095238\n",
      "Class 2.0 recall = 0.11412609736632083\n",
      "Class 2.0 F1 Measure = 0.1874180865006553\n",
      "______________________Horror_____________________\n",
      "Class 3.0 precision = 0.7619047619047619\n",
      "Class 3.0 recall = 0.02069857697283312\n",
      "Class 3.0 F1 Measure = 0.04030226700251889\n",
      "______________________Thriller_____________________\n",
      "Class 4.0 precision = 0.0\n",
      "Class 4.0 recall = 0.0\n",
      "Class 4.0 F1 Measure = 0.0\n",
      "______________________Romance_____________________\n",
      "Class 5.0 precision = 0.0\n",
      "Class 5.0 recall = 0.0\n",
      "Class 5.0 F1 Measure = 0.0\n",
      "[3003. 2262. 1253.  773.  545.  153.]\n",
      "Accuracy for label 0: 0.9137529137529138\n",
      "Accuracy for label 1: 0.14058355437665782\n",
      "Accuracy for label 2: 0.11412609736632083\n",
      "Accuracy for label 3: 0.02069857697283312\n",
      "Accuracy for label 4: 0.0\n",
      "Accuracy for label 5: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print_statistis(predictions,mapp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0d23ab-e52b-47ad-b191-cd6361dae97d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8118179-8537-405d-9aa0-43cfac6bdfe4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
